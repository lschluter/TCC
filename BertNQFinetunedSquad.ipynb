{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlc9RGlIEhoJgEhy41F7bt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lschluter/TCC/blob/main/BertNQFinetunedSquad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7xM_fsRdSk_"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "id": "ENk2MnwEqjuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "id": "dKtdevVadKp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seção para o deepSpeed"
      ],
      "metadata": {
        "id": "ygnJH9AtkOMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nvidia-pyindex"
      ],
      "metadata": {
        "id": "cEEV75xFiYDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepspeed"
      ],
      "metadata": {
        "id": "nI8rd7t9iRdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys; print('python:',sys.version)\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "import torch; print('Pytorch:',torch.__version__)\n",
        "\n",
        "import transformers; print('transformers:',transformers.__version__)\n",
        "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
        "import datasets; print('datasets:',datasets.__version__)\n",
        "\n",
        "import deepspeed; print('deepspeed:',deepspeed.__version__)"
      ],
      "metadata": {
        "id": "ZW4ULyU2kLMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_gpus = 1 # run the script on only one gpu\n",
        "gpu = 0 # select the gpu"
      ],
      "metadata": {
        "id": "8fd3lG_VlrJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seçao especifica pra treino"
      ],
      "metadata": {
        "id": "kT6QT5Q8kLyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "nQzEqk3EYw-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "38k1fkpqZtJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui podemos trocar o model_checkpoint pro que desejarmos, mas o modelo que for posto aqui será ajustado ao dataset Squad"
      ],
      "metadata": {
        "id": "U-zWFCxL8sVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"bert-base-uncased\"\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "q63K9eMQZ7CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "datasets = load_dataset('squad', cache_dir=\"/content/cache\")"
      ],
      "metadata": {
        "id": "vtAyEKfKaS5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "3ApD_qIvaqKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "metadata": {
        "id": "ZMeKUqe9askc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "pad_on_right = tokenizer.padding_side == \"right\""
      ],
      "metadata": {
        "id": "BNBtNYZ6axJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa função a seguir foi baseada em: https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
        "\n",
        "Mas algumas alterações tiveram que ser feitas para funcionar no nosso caso."
      ],
      "metadata": {
        "id": "LvIi1nY_8_SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "_VXuRaXUayES"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = prepare_train_features(datasets['train'][:5])"
      ],
      "metadata": {
        "id": "7yW1PNDyb5Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "Y70UbiRBcNh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "_APwbJA8cf-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criar arquivo local com nome dsConfig.json e conteúdo abaixo:\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"allgather_partitions\": true,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": true,\n",
        "        \"reduce_scatter\": true,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ],
      "metadata": {
        "id": "nBLwOGyDt_zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    \"bert-base-uncased-finetuned-nq\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        "    deepspeed=\"/content/ds_config.json\",\n",
        ")"
      ],
      "metadata": {
        "id": "-FKH66BMciQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate.utils import DistributedType\n",
        "args.distributed_state.distributed_type = DistributedType.DEEPSPEED"
      ],
      "metadata": {
        "id": "XjYcaaxaqXW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ACCELERATE_USE_DEEPSPEED=true"
      ],
      "metadata": {
        "id": "dxGZL_4Wp2mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ],
      "metadata": {
        "id": "_hh2-G0Cck-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "mF6JrzW0c3Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "A9RFKN8_c4tP",
        "outputId": "0a56ad97-d901-45d2-b443-d6c84e55af8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module cpu_adam, skipping build step...\n",
            "Loading extension module cpu_adam...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to load cpu_adam op: 2.2529258728027344 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py:1879: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8524' max='16599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8524/16599 3:23:55 < 3:13:13, 0.70 it/s, Epoch 1.54/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.008700</td>\n",
              "      <td>0.990913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16599' max='16599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16599/16599 6:39:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.008700</td>\n",
              "      <td>0.990913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.763500</td>\n",
              "      <td>1.044018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.529000</td>\n",
              "      <td>1.202199</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=16599, training_loss=0.7798612252065407, metrics={'train_runtime': 23955.2887, 'train_samples_per_second': 11.086, 'train_steps_per_second': 0.693, 'total_flos': 5.204482559521587e+16, 'train_loss': 0.7798612252065407, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "S-e9--bBdCgR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c8c47e29-0c3e-4322-c0a1-0df417bb2952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/leonardoschluter/bert-base-uncased-finetuned-nq-finetuned-squad/tree/main/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3pEd1J0NfoS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}